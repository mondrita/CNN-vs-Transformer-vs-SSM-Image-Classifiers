{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":505351,"sourceType":"datasetVersion","datasetId":174469}],"dockerImageVersionId":30840,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport torch.nn.functional as F\nfrom torch.utils.data import DataLoader,random_split\nfrom sklearn.model_selection import train_test_split\nfrom torch.utils.data import DataLoader, Subset, WeightedRandomSampler\n\nfrom torchvision import transforms, datasets\n\nfrom sklearn.metrics import (\n    roc_curve,\n    auc,\n    confusion_matrix,\n    precision_score,\n    recall_score,\n    f1_score\n)\nfrom sklearn.utils import class_weight\n\nfrom PIL import Image\nfrom torch.optim.lr_scheduler import StepLR\nimport random\nimport time\nfrom tqdm import tqdm\nfrom collections import Counter\nfrom sklearn.utils.class_weight import compute_class_weight","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"seed = 42\ntorch.manual_seed(seed)           \nrandom.seed(seed)                  \nnp.random.seed(seed)               \ntorch.cuda.manual_seed(seed)       \ntorch.backends.cudnn.deterministic = True  \ntorch.backends.cudnn.benchmark = False","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# **TENSORBOARD SETUP**","metadata":{}},{"cell_type":"code","source":"pip install tensorboard","metadata":{"trusted":true,"execution":{"execution_failed":"2025-01-15T19:32:27.43Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**Note: change the name of your directory. For example, if your model's name is \"Deit\" then log_dir=\"runs/deit\". *Do not change the main directory \"runs\"!***","metadata":{}},{"cell_type":"code","source":"from torch.utils.tensorboard import SummaryWriter\n\nwriter = SummaryWriter(log_dir=\"runs/ConvNextv2\") ","metadata":{"trusted":true,"execution":{"execution_failed":"2025-01-15T19:32:27.43Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# **DATA LOADING & PREPROCESSING**","metadata":{}},{"cell_type":"code","source":"train_data_path = '/kaggle/input/train'\ntest_data_path = '/kaggle/input/test'","metadata":{"trusted":true,"execution":{"execution_failed":"2025-01-15T19:32:27.43Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Load dataset\ntrain_dataset = datasets.ImageFolder(root=train_data_path)\n\n# Print class-to-index mapping\nprint(\"Class-to-Index Mapping:\", train_dataset.class_to_idx)\n\n# Get counts of each label\nlabel_counts = Counter([sample[1] for sample in train_dataset.samples])\n\n# Print counts for each label\nprint(\"Label Counts:\")\nfor label, count in label_counts.items():\n    class_name = list(train_dataset.class_to_idx.keys())[list(train_dataset.class_to_idx.values()).index(label)]\n    print(f\"{class_name}: {count} images\")\n","metadata":{"trusted":true,"execution":{"execution_failed":"2025-01-15T19:32:27.43Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_transforms = transforms.Compose([\n    transforms.Resize((224, 224)),  # Resize\n    transforms.RandomHorizontalFlip(p=0.5),  # Horizontal flip\n    transforms.RandomVerticalFlip(p=0.5),  # Vertical flip\n    transforms.RandomRotation(degrees=10),  # Rotation\n    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2),  # Color jitter\n    transforms.GaussianBlur(kernel_size=(3, 3), sigma=(0.1, 0.5)),  # Gaussian noise\n    transforms.ToTensor(),  # Convert to Tensor\n    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])  # Normalize\n])\n\ntest_transforms = transforms.Compose([\n    transforms.Resize((224, 224)),  # Resize\n    transforms.ToTensor(),  # Convert to Tensor\n    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])  # Normalize\n])","metadata":{"trusted":true,"execution":{"execution_failed":"2025-01-15T19:32:27.431Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_dataset = datasets.ImageFolder(root=train_data_path , transform=train_transforms)\ntest_dataset = datasets.ImageFolder(root=test_data_path, transform=test_transforms)\n\n# Data loaders\ntrain_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, num_workers=2)\ntest_loader = DataLoader(test_dataset, batch_size=32, shuffle=False, num_workers=2)\n\nprint(f\"Training Set Size: {len(train_dataset)} images\")\nprint(f\"Testing Set Size: {len(test_dataset)} images\")","metadata":{"trusted":true,"execution":{"execution_failed":"2025-01-15T19:32:27.431Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_labels = [label for _, label in train_dataset.samples]\ntrain_label_counts = Counter(train_labels)\n\ntest_labels = [label for _, label in test_dataset.samples]\ntest_label_counts = Counter(test_labels)\n\nclass_names = train_dataset.classes\ntrain_label_names = {class_names[label]: count for label, count in train_label_counts.items()}\ntest_label_names = {class_names[label]: count for label, count in test_label_counts.items()}\n\n# Visualize label distributions\nprint(\"Training Label Distribution:\", train_label_names)\nprint(\"Testing Label Distribution:\", test_label_names)\n\ndef plot_label_distribution(label_counts, title):\n    labels, counts = zip(*label_counts.items())\n    plt.figure(figsize=(4, 2))\n    plt.bar(labels, counts)\n    plt.xlabel('Class Labels')\n    plt.ylabel('Number of Samples')\n    plt.title(title)\n    plt.xticks(rotation=45)\n    plt.show()\n\nplot_label_distribution(train_label_names, \"Training Dataset Label Distribution\")\nplot_label_distribution(test_label_names, \"Testing Dataset Label Distribution\")","metadata":{"trusted":true,"execution":{"execution_failed":"2025-01-15T19:32:27.431Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# **LOAD MODEL**","metadata":{}},{"cell_type":"markdown","source":"**Note: Load your model here**","metadata":{}},{"cell_type":"code","source":"from timm import create_model\nimport torch.nn as nn\n\n# Load the ConvNeXtV2 model\nmodel = create_model(\"convnextv2_tiny.fb_in1k\", pretrained=True)\n\n# Modify the classification head for binary classification (2 classes)\nnum_features = model.head.fc.in_features  # Access the input size of the fully connected layer\nmodel.head.fc = nn.Linear(num_features, 2)  # Replace with a new fully connected layer\n\n# Send model to GPU if available\ndevice = 'cuda' if torch.cuda.is_available() else 'cpu'\nmodel = model.to(device)","metadata":{"trusted":true,"execution":{"execution_failed":"2025-01-15T19:32:27.431Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"labels = [label for _, label in train_dataset.samples] \nclass_weights = compute_class_weight(class_weight='balanced', classes=np.unique(labels), y=labels)\nclass_weights = torch.tensor(class_weights, dtype=torch.float).to(device)\n\ncriterion = nn.CrossEntropyLoss(weight=class_weights)\noptimizer = optim.AdamW(model.parameters(), lr=1e-4, weight_decay=1e-4)\nscheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', patience=5, factor=0.1, verbose=True)","metadata":{"trusted":true,"execution":{"execution_failed":"2025-01-15T19:32:27.431Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# **TRAINING**","metadata":{}},{"cell_type":"code","source":"class EarlyStopping:\n    def __init__(self, patience=5, delta=0.0):\n        self.patience = patience\n        self.delta = delta\n        self.best_loss = float('inf')\n        self.counter = 0\n        self.early_stop = False\n\n    def __call__(self, train_loss):\n        if train_loss < self.best_loss - self.delta:\n            self.best_loss = train_loss\n            self.counter = 0\n        else:\n            self.counter += 1\n            if self.counter >= self.patience:\n                print(f\"Early stopping triggered after {self.patience} epochs with no improvement.\")\n                self.early_stop = True","metadata":{"trusted":true,"execution":{"execution_failed":"2025-01-15T19:32:27.431Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def plot_training_results(train_losses, train_accuracies, learning_rates, fps, gpu_usage):\n    # Learning Rate\n    plt.figure(figsize=(8, 6))\n    plt.plot(range(1, len(learning_rates) + 1), learning_rates, marker='o')\n    plt.xlabel(\"Epochs\")\n    plt.ylabel(\"Learning Rate\")\n    plt.title(\"Learning Rate Schedule\")\n    plt.grid()\n    plt.show()\n\n    # Training Accuracy\n    plt.figure(figsize=(8, 6))\n    plt.plot(range(1, len(train_accuracies) + 1), train_accuracies, label=\"Training Accuracy\", marker='o')\n    plt.xlabel(\"Epochs\")\n    plt.ylabel(\"Accuracy (%)\")\n    plt.title(\"Training Accuracy\")\n    plt.legend()\n    plt.grid()\n    plt.show()\n\n    # FPS\n    plt.figure(figsize=(8, 6))\n    plt.plot(range(1, len(fps) + 1), fps, marker='o')\n    plt.xlabel(\"Epochs\")\n    plt.ylabel(\"Frames Per Second (FPS)\")\n    plt.title(\"Training Speed (FPS)\")\n    plt.grid()\n    plt.show()\n\n    # GPU Usage\n    allocated, reserved = zip(*gpu_usage)\n    plt.figure(figsize=(8, 6))\n    plt.plot(range(len(allocated)), allocated, label=\"GPU Allocated (GB)\")\n    plt.plot(range(len(reserved)), reserved, label=\"GPU Reserved (GB)\")\n    plt.xlabel(\"Batches\")\n    plt.ylabel(\"GPU Memory (GB)\")\n    plt.title(\"GPU Memory Usage\")\n    plt.legend()\n    plt.grid()\n    plt.show()\n\ndef write_training_results(train_losses, train_accuracies, learning_rates, fps, gpu_usage):\n    # Learning Rate\n    plt.figure(figsize=(8, 6))\n    plt.plot(range(1, len(learning_rates) + 1), learning_rates, marker='o')\n    plt.xlabel(\"Epochs\")\n    plt.ylabel(\"Learning Rate\")\n    plt.title(\"Learning Rate Schedule\")\n    plt.grid()\n    fig = plt.gcf()\n    if writer:\n        writer.add_figure(\"Learning Rate Schedule\", fig)\n\n    # Training Accuracy\n    plt.figure(figsize=(8, 6))\n    plt.plot(range(1, len(train_accuracies) + 1), train_accuracies, label=\"Training Accuracy\", marker='o')\n    plt.xlabel(\"Epochs\")\n    plt.ylabel(\"Accuracy (%)\")\n    plt.title(\"Training Accuracy\")\n    plt.legend()\n    plt.grid()\n    fig = plt.gcf()\n    if writer:\n        writer.add_figure(\"Training Accuracy\", plt.gcf())\n\n    # FPS\n    plt.figure(figsize=(8, 6))\n    plt.plot(range(1, len(fps) + 1), fps, marker='o')\n    plt.xlabel(\"Epochs\")\n    plt.ylabel(\"Frames Per Second (FPS)\")\n    plt.title(\"Training Speed (FPS)\")\n    plt.grid()\n    fig = plt.gcf()\n    if writer:\n        writer.add_figure(\"Training Speed (FPS)\", fig)\n\n    # GPU Usage\n    allocated, reserved = zip(*gpu_usage)\n    plt.figure(figsize=(8, 6))\n    plt.plot(range(len(allocated)), allocated, label=\"GPU Allocated (GB)\")\n    plt.plot(range(len(reserved)), reserved, label=\"GPU Reserved (GB)\")\n    plt.xlabel(\"Batches\")\n    plt.ylabel(\"GPU Memory (GB)\")\n    plt.title(\"GPU Memory Usage\")\n    plt.legend()\n    plt.grid()\n    fig = plt.gcf()\n    if writer:\n        writer.add_figure(\"GPU Memory Usage\", fig)","metadata":{"trusted":true,"execution":{"execution_failed":"2025-01-15T19:32:27.431Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def train_model(model, train_loader, optimizer, scheduler, criterion, device, patience=5, num_epochs=50):\n    early_stopping = EarlyStopping(patience=patience)\n    model.to(device)\n\n    # Lists to track metrics\n    train_losses, train_accuracies, learning_rates = [], [], []\n    gpu_usage, fps = [], []\n\n    for epoch in range(num_epochs):\n        # Fetch and print the current learning rate\n        current_lr = optimizer.param_groups[0]['lr']\n        learning_rates.append(current_lr)  # Track learning rate\n        writer.add_scalar(\"Learning Rate\", current_lr, epoch)\n        \n        print(f\"\\nEpoch {epoch+1}/{num_epochs}, Learning Rate: {current_lr:.6f}\")\n\n        model.train()\n        train_loss, correct, total = 0.0, 0, 0\n\n        # GPU memory tracking\n        start_time = time.time()\n        for images, labels in tqdm(train_loader, desc=\"Training Batches\"):\n            images, labels = images.to(device), labels.to(device)\n            optimizer.zero_grad()\n\n            # Forward pass\n            outputs = model(images)\n\n            # Check for logits attribute (e.g., in DeiT models)\n            if hasattr(outputs, 'logits'):\n                outputs = outputs.logits\n\n            # Compute loss and backpropagate\n            loss = criterion(outputs, labels)\n            loss.backward()\n            optimizer.step()\n\n            train_loss += loss.item()\n            _, predicted = outputs.max(1)\n            total += labels.size(0)\n            correct += predicted.eq(labels).sum().item()\n\n        # Monitor GPU memory usage\n        gpu_allocated = torch.cuda.memory_allocated(device) / (1024 ** 3)  # Convert to GB\n        gpu_reserved = torch.cuda.memory_reserved(device) / (1024 ** 3)   \n        gpu_usage.append((gpu_allocated, gpu_reserved))\n\n        epoch_loss = train_loss / len(train_loader)\n        epoch_acc = 100. * correct / total\n        train_losses.append(epoch_loss)  # Track training loss\n        train_accuracies.append(epoch_acc)  # Track training accuracy\n\n        # Log training loss and accuracy to TensorBoard\n        writer.add_scalar(\"Loss/Train\", epoch_loss, epoch)\n        writer.add_scalar(\"Accuracy/Train\", epoch_acc, epoch)\n\n        # Measure FPS\n        end_time = time.time()\n        fps_epoch = len(train_loader.dataset) / (end_time - start_time)\n        fps.append(fps_epoch)\n\n        print(f\"Training Loss: {epoch_loss:.4f}, Training Accuracy: {epoch_acc:.2f}%, FPS: {fps_epoch:.2f}\")\n        print(f\"GPU Memory Allocated: {gpu_allocated:.2f} GB, GPU Memory Reserved: {gpu_reserved:.2f} GB\")\n\n        # Log FPS\n        writer.add_scalar(\"Training/FPS\", fps_epoch, epoch)\n\n        # Step the LR scheduler\n        scheduler.step(epoch_loss)\n\n        # Early Stopping Check\n        early_stopping(epoch_loss)\n        if early_stopping.early_stop:\n            print(\"Stopping early due to convergence.\")\n            break\n\n    # Plot Results\n    plot_training_results(train_losses, train_accuracies, learning_rates, fps, gpu_usage)\n    write_training_results(train_losses, train_accuracies, learning_rates, fps, gpu_usage)","metadata":{"trusted":true,"execution":{"execution_failed":"2025-01-15T19:32:27.431Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_model(\n    model=model,\n    train_loader=train_loader,\n    optimizer=optimizer,\n    scheduler=scheduler,\n    criterion=criterion,\n    device=device,\n    patience=5,  \n    num_epochs=50\n)","metadata":{"trusted":true,"execution":{"execution_failed":"2025-01-15T19:32:27.431Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# **EVALUATION**","metadata":{}},{"cell_type":"code","source":"def evaluate_model_with_metrics(model, loader, binary_classification=True):\n    model.eval()\n    all_preds = []\n    all_labels = []\n    all_probs = []  \n    correct = 0\n    total = 0\n\n    with torch.no_grad():\n        for images, labels in loader:\n            images, labels = images.to('cuda'), labels.to('cuda')\n            \n            # Get model outputs\n            outputs = model(images)\n            \n            # For DeiT models, extract logits\n            if hasattr(outputs, 'logits'):  \n                outputs = outputs.logits\n            \n            # Get predicted classes\n            _, predicted = outputs.max(1)\n\n            # Accumulate metrics\n            total += labels.size(0)\n            correct += predicted.eq(labels).sum().item()\n            all_preds.extend(predicted.cpu().numpy())\n            all_labels.extend(labels.cpu().numpy())\n\n            # Probabilities for ROC AUC\n            if binary_classification:\n                probabilities = F.softmax(outputs, dim=1)[:, 1].cpu().numpy()\n                all_probs.extend(probabilities)\n\n    # Accuracy\n    accuracy = 100. * correct / total\n    print(f\"Accuracy on test set: {accuracy:.2f}%\")\n    writer.add_scalar(\"Evaluation/Accuracy\", accuracy)\n\n    all_preds = np.array(all_preds)\n    all_labels = np.array(all_labels)\n\n    # Confusion Matrix\n    cm = confusion_matrix(all_labels, all_preds)\n    print(\"Confusion Matrix:\\n\", cm)\n\n    plt.figure(figsize=(8, 6))\n    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=np.unique(all_labels),\n                yticklabels=np.unique(all_labels), cbar=False)\n    plt.xlabel('Predicted Labels')\n    plt.ylabel('True Labels')\n    plt.title('Confusion Matrix')\n    fig = plt.gcf()\n    if writer:\n        writer.add_figure(\"Evaluation/Confusion Matrix\", fig)\n\n    # Precision, Recall, F1-Score\n    precision = precision_score(all_labels, all_preds, average='binary' if binary_classification else 'weighted')\n    recall = recall_score(all_labels, all_preds, average='binary' if binary_classification else 'weighted')\n    f1 = f1_score(all_labels, all_preds, average='binary' if binary_classification else 'weighted')\n    print(f\"Precision: {precision:.4f}\")\n    print(f\"Recall: {recall:.4f}\")\n    print(f\"F1-Score: {f1:.4f}\")\n    writer.add_scalar(\"Evaluation/Precision\", precision)\n    writer.add_scalar(\"Evaluation/Recall\", recall)\n    writer.add_scalar(\"Evaluation/F1-Score\", f1)\n\n    # AUC-ROC and ROC Curve (for binary classification)\n    if binary_classification:\n        all_probs = np.array(all_probs)\n        if len(np.unique(all_labels)) == 2: \n            fpr, tpr, _ = roc_curve(all_labels, all_probs)\n            auc_score = auc(fpr, tpr)\n            print(f\"AUC-ROC: {auc_score:.4f}\")\n\n            # Plot ROC Curve\n            plt.figure()\n            plt.plot(fpr, tpr, color='blue', lw=2, label=f'ROC curve (AUC = {auc_score:.4f})')\n            plt.plot([0, 1], [0, 1], color='red', linestyle='--', lw=2)\n            plt.xlabel('False Positive Rate')\n            plt.ylabel('True Positive Rate')\n            plt.title('Receiver Operating Characteristic (ROC) Curve')\n            plt.legend(loc=\"lower right\")\n            plt.grid()\n            fig = plt.gcf()\n            if writer:\n                writer.add_figure(\"Evaluation/ROC Curve\", fig)\n        else:\n            print(\"AUC-ROC is not supported for multi-class tasks without modification.\")","metadata":{"trusted":true,"execution":{"execution_failed":"2025-01-15T19:32:27.431Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"evaluate_model_with_metrics(model, test_loader, binary_classification=True)","metadata":{"trusted":true,"execution":{"execution_failed":"2025-01-15T19:32:27.431Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def plot_confusion_roc(model, loader, binary_classification=True):\n    model.eval()\n    all_preds = []\n    all_labels = []\n    all_probs = [] \n\n    with torch.no_grad():\n        for images, labels in loader:\n            images, labels = images.to('cuda'), labels.to('cuda')\n            outputs = model(images)\n\n            if hasattr(outputs, 'logits'):\n                outputs = outputs.logits\n\n            _, predicted = outputs.max(1)\n            all_preds.extend(predicted.cpu().numpy())\n            all_labels.extend(labels.cpu().numpy())\n\n            # For ROC AUC\n            if binary_classification:\n                probabilities = F.softmax(outputs, dim=1)[:, 1].cpu().numpy()\n                all_probs.extend(probabilities)\n\n    all_preds = np.array(all_preds)\n    all_labels = np.array(all_labels)\n\n    # Confusion Matrix\n    cm = confusion_matrix(all_labels, all_preds)\n    plt.figure(figsize=(8, 6))\n    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=np.unique(all_labels),\n                yticklabels=np.unique(all_labels), cbar=False)\n    plt.xlabel('Predicted Labels')\n    plt.ylabel('True Labels')\n    plt.title('Confusion Matrix')\n    plt.show()\n\n    # ROC Curve (for binary classification)\n    if binary_classification:\n        fpr, tpr, _ = roc_curve(all_labels, all_probs)\n        auc_score = auc(fpr, tpr)\n\n        plt.figure()\n        plt.plot(fpr, tpr, color='blue', lw=2, label=f'ROC curve (AUC = {auc_score:.4f})')\n        plt.plot([0, 1], [0, 1], color='red', linestyle='--', lw=2)\n        plt.xlabel('False Positive Rate')\n        plt.ylabel('True Positive Rate')\n        plt.title('Receiver Operating Characteristic (ROC) Curve')\n        plt.legend(loc=\"lower right\")\n        plt.grid()\n        plt.show()\n\nplot_confusion_roc(model, test_loader, binary_classification=True)","metadata":{"trusted":true,"execution":{"execution_failed":"2025-01-15T19:32:27.432Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# **COMPUTATIONAL EFFICIENCY METRIC**","metadata":{}},{"cell_type":"code","source":"pip install fvcore","metadata":{"trusted":true,"execution":{"execution_failed":"2025-01-15T19:32:27.432Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nfrom torchvision.models import convnext_base, ConvNeXt_Base_Weights\nfrom torchinfo import summary\nimport time\nfrom prettytable import PrettyTable\nfrom fvcore.nn import FlopCountAnalysis, parameter_count_table\n","metadata":{"trusted":true,"execution":{"execution_failed":"2025-01-15T19:32:27.432Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"image, label = train_dataset[0]  # Use the first image from the dataset\nimage = image.unsqueeze(0).to(device)\ninput_tensor = image\nbatch_size = 1\n\nflops = FlopCountAnalysis(model, image)\ntotal_flops = flops.total() / 1e9  # Convert to GFLOPs\nparams_table = parameter_count_table(model)\ntotal_params = sum(p.numel() for p in model.parameters())\ntrainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n\n# Inference Time\nstart_time = time.time()\nfor _ in range(10):  # Run for 10 iterations\n    with torch.no_grad():\n        model(input_tensor)\nend_time = time.time()\nbatch_inference_time = (end_time - start_time) / 10\nper_image_inference_time = batch_inference_time / batch_size\n\n# GPU Memory Usage\nif device == 'cuda':\n    gpu_allocated = torch.cuda.memory_allocated(device) / (1024 ** 3)\n    gpu_reserved = torch.cuda.memory_reserved(device) / (1024 ** 3)\nelse:\n    gpu_allocated = gpu_reserved = 0.0\n\n# Throughput (FPS)\nthroughput = batch_size / batch_inference_time\n\nfrom prettytable import PrettyTable\ntable = PrettyTable()\ntable.field_names = [\"Metric\", \"Value\"]\ntable.add_row([\"FLOPs (GFLOPs)\", f\"{total_flops:.2f}\"])\ntable.add_row([\"Total Parameters\", f\"{total_params:,}\"])\ntable.add_row([\"Trainable Parameters\", f\"{trainable_params:,}\"])\ntable.add_row([\"Inference Time (Batch)\", f\"{batch_inference_time:.4f} seconds\"])\ntable.add_row([\"Inference Time (Per Image)\", f\"{per_image_inference_time:.4f} seconds\"])\ntable.add_row([\"Throughput (FPS)\", f\"{throughput:.2f} images/second\"])\ntable.add_row([\"GPU Memory Allocated\", f\"{gpu_allocated:.2f} GB\"])\ntable.add_row([\"GPU Memory Reserved\", f\"{gpu_reserved:.2f} GB\"])\n\nprint(table)\n\n# Log computational metrics to TensorBoard\nwriter.add_scalar(\"Efficiency/FLOPs (GFLOPs)\", total_flops)\nwriter.add_scalar(\"Efficiency/Total Parameters\", total_params)\nwriter.add_scalar(\"Efficiency/Inference Time (ms)\", per_image_inference_time * 1000)\nwriter.add_scalar(\"Efficiency/Throughput (FPS)\", throughput)\n","metadata":{"trusted":true,"execution":{"execution_failed":"2025-01-15T19:32:27.432Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"writer.close()","metadata":{"trusted":true,"execution":{"execution_failed":"2025-01-15T19:32:27.432Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# **SAVE MODEL & LOGS**","metadata":{}},{"cell_type":"code","source":"path_50epochs = '/kaggle/working/model.pth'\n\ntorch.save(model.state_dict(), path_50epochs)\n\nprint(\"Models saved successfully to /kaggle/working/\")","metadata":{"trusted":true,"execution":{"execution_failed":"2025-01-15T19:32:27.432Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!tar -czf tensorboard_logs.tar.gz runs","metadata":{"trusted":true,"execution":{"execution_failed":"2025-01-15T19:32:27.432Z"}},"outputs":[],"execution_count":null}]}